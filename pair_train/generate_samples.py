# Code to generate chimera augmentations using a trained network.
from os.path import join
import json
import itertools as it
import cPickle
import logging
from collections import defaultdict
import numpy as np

import torch

import utils.methods as mt
import nn as pairnn
import data as dataman
import gan

_formatstr = '%(asctime)s %(name)s %(levelname)s]: %(message)s'

# Set up the logger.
_MYLOGGER = None
def _setupLogger():
  global _MYLOGGER
  LOGLEVEL = logging.DEBUG
  _MYLOGGER = logging.getLogger(__name__)
  ch = logging.StreamHandler()
  ch.setLevel(LOGLEVEL)
  _FORMATTER = logging.Formatter(fmt=_formatstr)
  ch.setFormatter(_FORMATTER)
  _MYLOGGER.addHandler(ch)
  _MYLOGGER.setLevel(LOGLEVEL)
  _MYLOGGER.propagate = False

_setupLogger()

def getLogger():
  global _MYLOGGER
  return _MYLOGGER

class AugmentedDataSet(object):
  EXTENSION = ".ads"
  def __init__(self):
    self._data = []
  def addPoint(self, dataPoint, outputPoint):
    """
    dataPoint: a tuple (x,y), of points and expected outputs 
               ('y' is needed for similarity score)
    outputPoint: the features for this datapoint generated by the network
    """
    self._data.append((dataPoint, outputPoint))
  def addAds(self, ads):
    self._data += ads._data
  @property
  def data(self):
    return self._data
  def sanitize(self, fileName):
    if not fileName.endswith(self.EXTENSION):
      fileName += self.EXTENSION
    return fileName
  def save(self, outFileName):
    outFileName = self.sanitize(outFileName)
    torch.save(self._data, outFileName)
  def saveJson(self, out_filename):
    out_filename = self.sanitize(out_filename) + ".json"
    json.dump(self._data, open(out_filename, "w"))
  def load(self, inFileName):
    inFileName = self.sanitize(inFileName)
    self._data = torch.load(inFileName)

def generate_augmented_chimeras(
    modelFileName, pairFileName, sourceWhitelistName, featDir, gpu_id):
  """
  modelFileName - name of the neural network model file.
  pairFileName - name of the file with vrn pairings.
  sourceWhitelistName - name of a file containing a list of image names. Only
                        pairs in which the first image matches one of these
                        names will be accepted.
  """
  logger = getLogger()
  logger.debug("modelFileName=%s" % modelFileName)
  logger.debug("pairFileName=%s" % pairFileName)
  model = torch.load(modelFileName)
  vrnData = json.load(open(pairFileName))
  whitelists = set(json.load(open(sourceWhitelistName)))
  print "Making data set"
  print "length of whitelists: %d" % len(whitelists)
  dataSet = pairnn.makeDataSet("%s_model_" % modelFileName, featDir, vrnData, whitelists)
  print "Computing features"
  features = pairnn.computeAllFeatures(model, dataSet, gpu_id)

  ret = AugmentedDataSet()

  for i, (dataPoint, outputPoint) in enumerate(it.izip(dataSet, features)):
    if i % 10000 == 9999:
      print "iteration %d of %d" % (i + 1, len(dataSet))
    #print "dataPoint=%s" % str(dataPoint)
    #print "outputPoint=%s" % str(outputPoint)
    ret.addPoint(dataPoint, outputPoint)
  print "Done generating augmented chimeras!"
  return ret

def write_augmented_chimeras(outFileName, *args, **kwargs):
  chimeras = generate_augmented_chimeras(*args, **kwargs)
  print "saving chimeras to %s" % str(outFileName)
  chimeras.save(outFileName)

def chimera_stub():
  # Only runs chimera generation, assumes other files are present.
  outFileName = "data/chimeras.pik"
  modelFileName = "data/models/nn.pyt"
  pairFileName = pairnn.VRNDATA
  sourceWhitelistName = "%s_%s" % (pairnn.TORCHCOMPDEVDATA, "devImgNames.json")
  featDir = pairnn.COMPFEATDIR

  write_augmented_chimeras(outFileName, modelFileName, pairFileName, sourceWhitelistName, featDir)

def generateDepsAndChimeras():
  # One method to run to train the network and generate chimeras.
  # This requires that the relevant pairnn.COMPFEATDIR and pairnn.VRNDATA
  # already exist.

  # Set up variables
  logger = getLogger()
  modelType = "nn"
  modelFileName = "data/models/%s.pyt" % modelType
  featDir = pairnn.COMPFEATDIR
  pairFileName = pairnn.VRNDATA
  trainLoc = pairnn.TORCHCOMPTRAINDATA
  devLoc = pairnn.TORCHCOMPDEVDATA
  # This file contains the image names used as the source in chimera generation
  sourceWhitelistName = "%s_%s" % (devLoc, "devImgNames.json")  

  # Generate everything dont always regenerate
#  pairnn.makeData(trainLoc, devLoc, featDir, pairFileName)
  logger.info("Running model {0}".format(modelType))
  pairnn.runModel(modelFileName, modelType, trainLoc=trainLoc, devLoc=devLoc)
  logger.info("Writing Chimeras")
  logger.debug("modelFileName=%s" % modelFileName)
  logger.debug("pairFileName=%s" % pairFileName)
  write_augmented_chimeras("%s_chimeras" % modelFileName, modelFileName, pairFileName, sourceWhitelistName, featDir)

def make_save_chimeras(params):
    """
    @param params - a MultiganParameters object
    """
    dirconfig = params.dirconfig
    mt.makeDirIfNeeded(dirconfig.chimdir)
    pdm = dataman.PairDataManager(dirconfig.pairdir, dirconfig.featdir)
    netg_dir = join(dirconfig.multigandir, "trained_models")
    handler = dataman.ShardedDataHandler(netg_dir, ".gan")

    full_vrn_data = json.load(open(dirconfig.heldoutVrnDataName))
    sharded_vrn_data = {}
    for elem in full_vrn_data:
        tup = (pdm.noun2int[elem[4]], pdm.noun2int[elem[5]])
        if tup not in sharded_vrn_data:
            sharded_vrn_data[tup] = []
        # Only accept images for which we didn't train on the source image.
        if elem[1] in pdm.dev_img_names:
            sharded_vrn_data[tup].append(elem)

    ads = AugmentedDataSet()
    for nounpair in handler.iterNounPairs():
        netg_file = handler.keyToPath(nounpair)
        _, netg = torch.load(netg_file)
        vrn_data = sharded_vrn_data[tuple(nounpair)]
        ads.addAds(
                generate_gan_chimeras(netg, vrn_data, pdm, **params.kwargs))
    ads.save(dirconfig.chim_base_name)

def generate_gan_chimeras(netg, vrn_data, pdm, **kwargs):
    assert(isinstance(netg, gan.TrGanG)), "Invalid netg type"
    dataset = pairnn.get_dataset(
            vrn_data, pdm.role2int, pdm.noun2int, defaultdict(lambda:
            np.zeros(1024), pdm.name2feat))
    features = pairnn.computeAllFeaturesGan(netg.cuda(0), dataset, gpu_id=0, **kwargs)
    ret = AugmentedDataSet()
    for elem in it.izip(dataset, features):
        ret.addPoint(*elem)
    return ret
